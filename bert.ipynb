{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR7UbUd0UYOZW2uW8N+uYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnrdmnc/NER-NLP/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "qzoksaWhwXKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DeMkpEnwRGf",
        "outputId": "de929d9d-0d7f-4e33-9e54-28822a8fb964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.13.*\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.13.*) (0.16.1)\n",
            "Collecting tensorflow<2.14,>=2.13.0 (from tensorflow-text==2.13.*)\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m972.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (24.3.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.4.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.36.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.13.*) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.8.0->tensorflow-text==2.13.*)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tf-keras, tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "orbax-checkpoint 0.4.4 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\n",
            "pydantic 2.6.4 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0 tf-keras-2.15.0 typing-extensions-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -U \"tensorflow-text==2.13.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tf-models-official==2.13.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE8pMVgAwW4d",
        "outputId": "bb14d974-4b2f-4115-e436-095f95f1ebbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-models-official==2.13.*\n",
            "  Downloading tf_models_official-2.13.2-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (3.0.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (9.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.84.0)\n",
            "Collecting immutabledict (from tf-models-official==2.13.*)\n",
            "  Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.5.16)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.23.5)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.9.0.80)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (6.0.1)\n",
            "Collecting sacrebleu (from tf-models-official==2.13.*)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (0.1.99)\n",
            "Collecting seqeval (from tf-models-official==2.13.*)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (4.9.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.13.*)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-text~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.*) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (6.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2023.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (24.3.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.36.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.13.*) (2.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.*) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.13.*) (3.1.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.*) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.13.*)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official==2.13.*)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.13.*) (4.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.13.*) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.43.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (6.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (3.18.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (3.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (3.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.*) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.2.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=83d90447aa621b71d6aef76f3ace3cb6411abda1b03d1333b4a1c42dbd083750\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorflow-model-optimization, portalocker, immutabledict, colorama, sacrebleu, seqeval, tf-models-official\n",
            "Successfully installed colorama-0.4.6 immutabledict-4.2.0 portalocker-2.8.2 sacrebleu-2.4.1 seqeval-1.2.2 tensorflow-model-optimization-0.8.0 tf-models-official-2.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qUHnLOq0yHHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Importa il modulo per interagire con il sistema operativo\n",
        "import shutil  # Importa il modulo per operazioni di alto livello sui file\n",
        "\n",
        "import tensorflow as tf  # Importa TensorFlow, un framework per machine learning\n",
        "import tensorflow_hub as hub  # Importa TensorFlow Hub per moduli pre-addestrati\n",
        "import tensorflow_text as text  # Importa TensorFlow Text per il trattamento del testo\n",
        "from official.nlp import optimization  # Importa modulo per AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt  # Importa Matplotlib per la visualizzazione dei dati\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')  # Imposta il livello di log a 'ERROR' in TensorFlow"
      ],
      "metadata": {
        "id": "xuizvQgqyHWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsvUbnEkyfN6",
        "outputId": "ce1df175-e0dc-4e72-a6a8-a3e25ccb94bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizzeremo text_dataset_from_directory per creare un tf.data.Dataset etichettato.\n",
        "\n",
        "Il set di dati IMDB è già stato suddiviso in training e test, ma manca un set di convalida.\n",
        "Creiamo un set di convalida utilizzando una suddivisione 80:20 dei dati di training utilizzando l'argomento validation_split riportato di seguito.\n",
        "quando si utilizzano gli argomenti validation_split e subset, assicurarsi di specificare un seme casuale o di passare shuffle=False, in modo che le suddivisioni di convalida e addestramento non si sovrappongano"
      ],
      "metadata": {
        "id": "-xeFxlMHyn9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APj_y33ly3d5",
        "outputId": "f6361381-49ea-45e9-9dfc-04b9881e735d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test delle recensioni"
      ],
      "metadata": {
        "id": "1roQzOzCy_5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnpVs8pKzAGk",
        "outputId": "841af53f-dc18-47b7-ae20-85afb9bddf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "Label : 0 (neg)\n",
            "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "Label : 0 (neg)\n",
            "Review: b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
            "Label : 1 (pos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caricamento di modelli da TensorFlow Hub\n",
        "\n",
        "Per caricare modelli BERT da TensorFlow Hub e perfezionarli, puoi scegliere tra diverse opzioni:\n",
        "\n",
        "## Modelli BERT Disponibili:\n",
        "1. **Base BERT**: Include modelli come il BERT senza custodia e altri sette modelli con pesi addestrati dagli autori originali.\n",
        "2. **Piccoli BERT**: Offrono la stessa architettura generale di BERT ma con meno e/o blocchi Transformer più piccoli per esplorare compromessi tra velocità, dimensioni e qualità.\n",
        "3. **ALBERT**: Offre quattro dimensioni diverse di \"A Lite BERT\" che riducono le dimensioni del modello condividendo i parametri tra i livelli.\n",
        "4. **Esperti BERT**: Forniscono otto modelli con l'architettura base BERT ma con variazioni per diversi domini di pre-formazione.\n",
        "5. **Electra**: Simile a BERT ma pre-addestrato come discriminatore in una configurazione simile a una GAN.\n",
        "6. **BERT con Talking-Heads Attenzione e GELU recintato**: Offre miglioramenti al nucleo dell'architettura Transformer.\n",
        "\n",
        "## Suggerimenti per la Scelta del Modello:\n",
        "- Inizia con un BERT piccolo per una messa a punto più veloce.\n",
        "- Se desideri un modello piccolo con maggiore precisione, considera ALBERT.\n",
        "- Per una precisione superiore, opta per le classiche dimensioni BERT o per i recenti perfezionamenti come Electra, Talking Heads o Esperti BERT.\n",
        "\n",
        "## Ulteriori Risorse:\n",
        "- Esistono anche versioni più grandi dei modelli che offrono una maggiore precisione ma potrebbero richiedere risorse più elevate.\n",
        "- Per attività più complesse, puoi utilizzare i modelli su una collaborazione TPU seguendo le istruzioni fornite.\n",
        "\n",
        "Per provare diversi modelli, è sufficiente modificare l'URL tfhub.dev nel codice, poiché le differenze sono gestite nei SavedModels di TF Hub. Segui i link forniti per ulteriori dettagli e riferimenti alla letteratura di ricerca.\n"
      ],
      "metadata": {
        "id": "E5BZGKxZzNSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Ni-sTbjyzORx",
        "outputId": "9f0dc842-b0e3-4407-e583-1688ee4371f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing model**\n",
        "\n",
        "Il modello di preprocessing associato ai modelli BERT forniti da TensorFlow Hub è essenziale per trasformare gli input di testo in identificatori numerici di token e organizzarli in diversi Tensor prima di essere inseriti in BERT. Questo modello di preprocessing, implementato utilizzando TF ops dalla libreria TF.text, è fondamentale per preparare i dati in modo compatibile con l'architettura dei modelli BERT.\n",
        "\n",
        "È importante notare che il modello di preprocessing corretto è quello menzionato nella documentazione del modello BERT specifico che stai utilizzando. Questo modello di preprocessing è selezionato automaticamente quando si carica il modello BERT tramite TensorFlow Hub, garantendo una corretta trasformazione dei dati testuali in input numerici per il modello BERT.\n",
        "\n",
        "Per caricare correttamente il modello di preprocessing insieme al modello BERT per ottimizzazioni ulteriori, è consigliabile utilizzare hub.KerasLayer, che rappresenta l'API preferita per caricare un SavedModel TF2-style da TF Hub in un modello Keras.\n",
        "\n",
        "Se hai bisogno di ulteriori dettagli o chiarimenti sulla selezione e l'utilizzo del modello di preprocessing specifico per un determinato modello BERT, ti invito a consultare la documentazione ufficiale del modello BERT che stai utilizzando tramite il link fornito sopra.\n",
        "\n",
        "Citations:\n",
        "[1] https://www.tensorflow.org/hub\n",
        "[2] https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html?m=1\n",
        "[3] https://www.tensorflow.org/hub/tutorials/bert_experts\n",
        "[4] https://cloud.google.com/blog/topics/developers-practitioners/add-preprocessing-functions-tensorflow-models-and-deploy-vertex-ai\n",
        "[5] https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html"
      ],
      "metadata": {
        "id": "uUINukuF1d2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BERT Preprocessing Model in Google Colab**\n",
        "\n",
        "Il `bert_preprocess_model` rappresenta un layer di pre-elaborazione basato su BERT utilizzando TensorFlow Hub. Questo layer è fondamentale per preparare i dati in ingresso in modo da renderli compatibili con il modello BERT durante l'addestramento o l'inferenza.\n",
        "\n",
        "Nel dettaglio, il layer di pre-elaborazione BERT si occupa delle seguenti operazioni:\n",
        "- **Tokenizzazione del Testo**: Suddivide il testo in token significativi.\n",
        "- **Conversione in ID Numerici**: Trasforma i token in ID numerici per la rappresentazione numerica del testo.\n",
        "- **Aggiunta di Token Speciali**: Inserisce i token speciali come `[CLS]` e `[SEP]` richiesti da BERT.\n",
        "- **Altre Trasformazioni**: Applica eventuali altre trasformazioni necessarie per adattare i dati al modello BERT.\n",
        "\n",
        "Utilizzando questo layer di pre-elaborazione, è possibile preparare correttamente i dati testuali affinché possano essere elaborati in modo efficiente da un modello BERT. Questo processo è essenziale per compiti come classificazione del testo, analisi del sentiment o altre attività di elaborazione del linguaggio naturale (NLP) su Google Colab.**testo in grassetto**"
      ],
      "metadata": {
        "id": "UM5VHB8w2kKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proviamo il modello di preelaborazione su del testo e vediamo l'output:"
      ],
      "metadata": {
        "id": "oaQsVHqc2qIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jcPjriIZCKxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "8JlXsvELCLFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "# Stampare le chiavi del dizionario restituito dal layer di pre-elaborazione\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "\n",
        "# Stampare la forma (shape) dell'input_word_ids\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "\n",
        "# Stampare gli ID delle parole per i primi 12 token\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "\n",
        "# Stampare la maschera di input per i primi 12 token\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "\n",
        "# Stampare gli ID del tipo per i primi 12 token\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a569kK03inV",
        "outputId": "d41f6d39-40af-4b20-ebd8-01948ebf8e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Output della Preelaborazione per un Modello BERT**\n",
        "Dopo la preelaborazione, il modello BERT utilizzerebbe tre output principali:\n",
        "input_word_ids: identificatori numerici dei token.\n",
        "\n",
        "1.   input_mask: maschera di input per gestire la lunghezza variabile delle sequenze.\n",
        "2.   input_type_ids: identificatori del tipo di input, che in questo caso hanno un solo valore (0) poiché si tratta di un input a frase singola.\n",
        "3. Altri punti rilevanti includono:\n",
        "L'input viene troncato a 128 token, ma questo numero può essere personalizzato.\n",
        "Per ulteriori dettagli sull'utilizzo di BERT per risolvere le attività GLUE su TPU, consulta la sezione Solve GLUE tasks using BERT on TPU Text TensorFlow.\n",
        "\n",
        "Gli input_type_ids contengono solo un valore (0) poiché l'input è una singola frase. Per input con più frasi, ci sarebbe un valore distinto per ciascuna.\n",
        "\n",
        "Essendo un modello TensorFlow, questo preprocessore di testo può essere facilmente integrato direttamente nel modello."
      ],
      "metadata": {
        "id": "e1IrPK7Y3unw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizzando il modello BERT\n",
        "\n",
        "Prima di inserire BERT nel tuo modello, diamo un'occhiata ai suoi risultati.\n",
        "Lo caricherai da TF Hub e vedrai i valori restituiti.\n",
        "\n"
      ],
      "metadata": {
        "id": "aLT9jYmu6KKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "CYs6ZsJ06T7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder) carica un modello BERT pre-addestrato utilizzando TensorFlow Hub e lo assegna alla variabile bert_model.\n",
        "\n",
        "Questo modello BERT pre-addestrato può essere utilizzato per eseguire operazioni di elaborazione del linguaggio naturale (NLP) come classificazione del testo, analisi del sentiment, generazione di testo e altre attività NLP.\n",
        "\n",
        "Utilizzando hub.KerasLayer, il modello BERT viene integrato come un layer all'interno di un modello Keras, consentendo di sfruttare le funzionalità di TensorFlow Hub per l'importazione e l'utilizzo semplice di modelli pre-addestrati come BERT all'interno di progetti di machine learning e deep learning."
      ],
      "metadata": {
        "id": "oiCIlk0W6xcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "# Stampa informazioni sui risultati ottenuti dall'elaborazione con il modello BERT\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape: {bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values: {bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape: {bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values: {bert_results[\"sequence_output\"][0, :12]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_J-_xm33uy8",
        "outputId": "393df2bc-237e-431f-c3c5-dd41cffa3af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape: (1, 512)\n",
            "Pooled Outputs Values: [ 0.762629    0.99280983 -0.18611868  0.36673862  0.15233733  0.6550447\n",
            "  0.9681154  -0.9486271   0.00216128 -0.9877732   0.06842692 -0.97630584]\n",
            "Sequence Outputs Shape: (1, 128, 512)\n",
            "Sequence Outputs Values: [[-0.28946346  0.3432128   0.33231518 ...  0.21300825  0.7102068\n",
            "  -0.05771117]\n",
            " [-0.28742072  0.31981036 -0.23018576 ...  0.58455    -0.21329743\n",
            "   0.72692114]\n",
            " [-0.66157067  0.68876773 -0.8743301  ...  0.1087725  -0.26173177\n",
            "   0.47855407]\n",
            " ...\n",
            " [-0.2256118  -0.2892561  -0.0706445  ...  0.47566038  0.83277136\n",
            "   0.40025333]\n",
            " [-0.2982428  -0.27473134 -0.05450517 ...  0.48849747  1.0955354\n",
            "   0.18163396]\n",
            " [-0.44378242  0.00930811  0.07223688 ...  0.1729009   1.1833243\n",
            "   0.07898017]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I modelli BERT restituiscono una mappa con 3 chiavi importanti: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
        "\n",
        "- `pooled_output` rappresenta ogni sequenza di input nel suo complesso. La forma è [batch_size, H]. Puoi pensare a questo come un embedding per l'intera recensione del film.\n",
        "- `sequence_output` rappresenta ogni token di input nel contesto. La forma è [batch_size, lunghezza_seq, H]. Puoi pensare a questo come un embedding contestuale per ogni token nella recensione del film.\n",
        "- Gli `encoder_outputs` sono le attivazioni intermedie dei blocchi Transformer L. `outputs[\"encoder_outputs\"][i]` è un tensore di forma [batch_size, lunghezza_seq, 1024] con le uscite dell'i-esimo blocco Transformer, per 0 <= i < L. L'ultimo valore della lista è uguale a `sequence_output`.\n",
        "Per il fine-tuning, utilizzerai l'array `pooled_output`."
      ],
      "metadata": {
        "id": "mmTTX0Fx7GiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definisci il tuo modello**\n",
        "\n",
        "Creerai un modello molto semplice per il fine-tuning, utilizzando il modello di pre-elaborazione, il modello BERT selezionato, uno strato Dense e uno strato Dropout.\n",
        "\n",
        "Nota: per ulteriori informazioni sull'input e l'output del modello di base, puoi seguire l'URL del modello per la documentazione. In questo caso specifico, non è necessario preoccuparsene poiché il modello di pre-elaborazione si occuperà di questo per te."
      ],
      "metadata": {
        "id": "5BL6M2yl7Ss3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "    # Definizione dell'input per il testo\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "\n",
        "    # Layer di pre-elaborazione basato su BERT\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "\n",
        "    # Elaborazione del testo attraverso il layer di pre-elaborazione\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "\n",
        "    # Caricamento del modello BERT come encoder\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "\n",
        "    # Passaggio dei dati attraverso il modello BERT\n",
        "    outputs = encoder(encoder_inputs)\n",
        "\n",
        "    # Estrazione dell'output aggregato (pooled output) dal modello BERT\n",
        "    net = outputs['pooled_output']\n",
        "\n",
        "    # Applicazione di un layer di Dropout per la regolarizzazione\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "    # Aggiunta di un layer Dense per la classificazione binaria senza attivazione\n",
        "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "\n",
        "    # Creazione del modello finale con input e output definiti\n",
        "    return tf.keras.Model(text_input, net)\n"
      ],
      "metadata": {
        "id": "pQiKHo9l7iPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Classifier Model\n",
        "\n",
        "`build_classifier_model` che costruisce un modello di classificazione utilizzando BERT come encoder. Ecco una spiegazione dettagliata delle operazioni svolte all'interno della funzione:\n",
        "\n",
        "1. Viene definito un input per il testo utilizzando `tf.keras.layers.Input`.\n",
        "2. Viene aggiunto un layer di pre-elaborazione basato su BERT utilizzando `hub.KerasLayer`.\n",
        "3. Il testo in input viene elaborato attraverso il layer di pre-elaborazione.\n",
        "4. Viene caricato il modello BERT come encoder utilizzando `hub.KerasLayer`.\n",
        "5. I dati vengono passati attraverso il modello BERT per ottenere gli output.\n",
        "6. Viene estratto l'output aggregato (pooled output) dal modello BERT.\n",
        "7. Viene applicato un layer di Dropout per la regolarizzazione.\n",
        "8. Viene aggiunto un layer Dense per la classificazione binaria senza attivazione.\n",
        "9. Infine, viene creato e restituito il modello finale con gli input e gli output definiti.\n",
        "\n",
        "Questo codice rappresenta la costruzione di un modello di classificazione che utilizza BERT come parte integrante per l'elaborazione del testo e la generazione delle previsioni."
      ],
      "metadata": {
        "id": "SrITmiXY7jHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifichiamo che il modello venga eseguito con l'output del modello di preelaborazione."
      ],
      "metadata": {
        "id": "jfdlra5M8HyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Costruzione del modello di classificazione utilizzando BERT come encoder\n",
        "classifier_model = build_classifier_model()\n",
        "\n",
        "# Esecuzione del modello sul testo di esempio per ottenere i risultati grezzi da BERT\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "\n",
        "# Applicazione della funzione di attivazione sigmoid per ottenere le previsioni finali\n",
        "print(tf.sigmoid(bert_raw_result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YluK2FHB71cQ",
        "outputId": "a489e5b7-c288-4bee-cf8a-ca383825d151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.36441654]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Viene costruito il modello di classificazione utilizzando BERT come encoder tramite la funzione `build_classifier_model`.\n",
        "2. Il modello viene eseguito sul testo di esempio `text_test` per ottenere i risultati grezzi generati da BERT.\n",
        "3. Viene applicata la funzione di attivazione sigmoid a `bert_raw_result` per ottenere le previsioni finali del modello.\n",
        "\n",
        "In sintesi, questo codice utilizza il modello di classificazione costruito con BERT per generare previsioni su un testo di esempio e applica la funzione di attivazione sigmoid per ottenere le probabilità associate alle classi di output."
      ],
      "metadata": {
        "id": "saWcJHV-75ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'output non ha significato, ovviamente, perché il modello non è stato ancora addestrato.\n",
        "\n",
        "Diamo un'occhiata alla struttura del modello."
      ],
      "metadata": {
        "id": "3tC2_ySu8POy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "udIB_W888Wgn",
        "outputId": "6ec311fa-d501-456e-fc75-3dbdd1fce2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa istruzione genererà un diagramma del modello classifier_model che mostrerà la struttura del modello con i vari strati e le connessioni tra di essi."
      ],
      "metadata": {
        "id": "o9SOVQHT8PQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "\n",
        "Ora disponi di tutti gli elementi per addestrare un modello, inclusi il modulo di preelaborazione, il codificatore BERT, i dati e il classificatore."
      ],
      "metadata": {
        "id": "s6Yoahho8wDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcolo della funzione di perdita\n",
        "\n",
        "Poiché si tratta di un problema di classificazione binaria e il modello restituisce una probabilità (uno strato a unità singola), utilizzerai la funzione di perdita di loss.BinaryCrossentropy."
      ],
      "metadata": {
        "id": "mPfkLeXP86wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definizione della funzione di loss come Binary Cross Entropy con from_logits=True\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Definizione della metrica di valutazione come Binary Accuracy\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "AAycXtd59LNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Binary Cross Entropy Loss: La funzione di loss BinaryCrossentropy viene definita con from_logits=True, indicando che gli output del modello sono logits e non probabilità. Questa loss è comunemente utilizzata per problemi di classificazione binaria.\n",
        "2.   Binary Accuracy Metric: La metrica di valutazione BinaryAccuracy viene definita per calcolare l'accuratezza della classificazione binaria. Questa metrica misura la percentuale di previsioni corrette rispetto alle etichette di classe binarie\n"
      ],
      "metadata": {
        "id": "5RfjBt5L86x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ottimizzatore\n",
        "\n",
        "Per il fine-tuning, utilizzeremo lo stesso ottimizzatore con cui è stato originariamente addestrato BERT: l'ottimizzatore \"Adaptive Moments\" (Adam). Questo ottimizzatore minimizza la perdita di previsione e applica la regolarizzazione tramite decadimento dei pesi (senza utilizzare i momenti), noto anche come [AdamW](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "Per il tasso di apprendimento (`init_lr`), utilizzeremo lo stesso schema di BERT durante la pre-elaborazione: decadimento lineare di un tasso di apprendimento iniziale ipotetico, preceduto da una fase di riscaldamento lineare nei primi 10% dei passaggi di addestramento (`num_warmup_steps`). In linea con il paper su BERT, il tasso di apprendimento iniziale è più basso per il fine-tuning (migliore tra 5e-5, 3e-5, 2e-5)."
      ],
      "metadata": {
        "id": "rfFB7Pdx9tdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definizione del numero di epoche per l'addestramento\n",
        "epochs = 2\n",
        "\n",
        "# Calcolo del numero di passaggi per epoca utilizzando la cardinalità del dataset di addestramento\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "\n",
        "# Calcolo del numero totale di passaggi di addestramento\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "\n",
        "# Calcolo del numero di passaggi di riscaldamento (warmup steps)\n",
        "num_warmup_steps = int(0.1 * num_train_steps)\n",
        "\n",
        "# Inizializzazione del tasso di apprendimento (learning rate)\n",
        "init_lr = 3e-5\n",
        "\n",
        "# Creazione dell'ottimizzatore utilizzando l'ottimizzatore AdamW e i parametri specificati\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n"
      ],
      "metadata": {
        "id": "xE-lQ9AO92I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Definizione delle Epoche**: Viene definito il numero di epoche per l'addestramento del modello.\n",
        "   \n",
        "2. **Calcolo dei Passaggi per Epoca**: Viene calcolato il numero di passaggi (batches) per ogni epoca utilizzando la cardinalità del dataset di addestramento.\n",
        "   \n",
        "3. **Calcolo dei Passaggi Totali di Addestramento**: Viene calcolato il numero totale di passaggi di addestramento moltiplicando i passaggi per epoca per il numero di epoche.\n",
        "   \n",
        "4. **Calcolo dei Passaggi di Riscaldamento**: Viene calcolato il numero di passaggi di riscaldamento (warmup steps) come il 10% dei passaggi totali.\n",
        "   \n",
        "5. **Inizializzazione del Tasso di Apprendimento**: Viene inizializzato il tasso di apprendimento a 3e-5.\n",
        "   \n",
        "6. **Creazione dell'Ottimizzatore**: Viene creato un ottimizzatore utilizzando l'ottimizzatore AdamW con i parametri iniziali specificati, come il tasso di apprendimento iniziale, il numero totale di passaggi e i passaggi di riscaldamento.\n"
      ],
      "metadata": {
        "id": "l5yFhQlB96PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caricamento del modello BERT e addestramento\n",
        "\n",
        "Utilizzando il `classifier_model` creato in precedenza, puoi compilare il modello con la perdita, la metrica e l'ottimizzatore."
      ],
      "metadata": {
        "id": "fZIseaSq-C9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurazione del modello per l'addestramento\n",
        "classifier_model.compile(optimizer=optimizer,  # Specifica dell'ottimizzatore\n",
        "                         loss=loss,  # Specifica della funzione di loss\n",
        "                         metrics=metrics)  # Specifica delle metriche da monitorare durante l'addestramento\n"
      ],
      "metadata": {
        "id": "gVxJX3XJ-DT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il tempo di formazione varierà a seconda della complessità del modello BERT selezionato."
      ],
      "metadata": {
        "id": "LXvm4WED-DHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stampa dell'informazione relativa al modello BERT utilizzato per l'addestramento\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "\n",
        "# Addestramento del modello utilizzando i dati di addestramento e di validazione\n",
        "history = classifier_model.fit(x=train_ds,  # Dati di addestramento\n",
        "                               validation_data=val_ds,  # Dati di validazione\n",
        "                               epochs=epochs)  # Numero di epoche per l'addestramento\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h8rQwup-f2l",
        "outputId": "6fe17287-bfdf-421a-cd87-f4c8b93afdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Epoch 1/2\n",
            " 13/625 [..............................] - ETA: 1:39:29 - loss: 0.7623 - binary_accuracy: 0.5048"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addestramento del Modello:\n",
        "Il modello viene addestrato utilizzando i dati di addestramento train_ds e i dati di validazione val_ds per il numero specificato di epoche epochs. Durante l'addestramento, il modello utilizzerà l'ottimizzatore configurato, la funzione di loss e le metriche specificate per migliorare le prestazioni sulla task di classificazione binaria"
      ],
      "metadata": {
        "id": "cJlGDtjq-oRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valuta il modello\n",
        "\n",
        "Vediamo come si comporta il modello. Verranno restituiti due valori. Perdita (un numero che rappresenta l'errore, i valori più bassi sono migliori) e accuratezza."
      ],
      "metadata": {
        "id": "NI-kHQoW-whl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione delle prestazioni del modello sui dati di test\n",
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "# Stampa della loss e dell'accuratezza ottenute durante la valutazione\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "CCvzRxEY-3lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the accuracy and loss over time\n",
        "\n",
        "Basato sull'oggetto \"History\" restituito da \"model.fit()\". È possibile tracciare la perdita di addestramento e convalida per il confronto, nonché l'accuratezza dell'addestramento e della convalida:"
      ],
      "metadata": {
        "id": "6cX8H7Y3--Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estrazione delle metriche di addestramento e validazione dalla history del modello\n",
        "history_dict = history.history\n",
        "\n",
        "# Estrazione delle chiavi disponibili nella history (metriche)\n",
        "print(history_dict.keys())\n",
        "\n",
        "# Estrazione di loss e accuratezza per addestramento e validazione\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "# Creazione della sequenza di epoche per il plot\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Creazione della figura per il plot\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "# Plot della loss per addestramento e validazione\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot dell'accuratezza per addestramento e validazione\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n"
      ],
      "metadata": {
        "id": "uKxBBU2p_K9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questo grafico, le linee rosse rappresentano la perdita e l'accuratezza dell'addestramento, mentre le linee blu rappresentano la perdita e l'accuratezza della convalida."
      ],
      "metadata": {
        "id": "o-pj0izE_Q9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvataggio del modello"
      ],
      "metadata": {
        "id": "jezd8POp_Q7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definizione del nome del dataset e del percorso per salvare il modello\n",
        "dataset_name = 'imdb'\n",
        "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "# Salvataggio del modello addestrato nel percorso specificato\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)\n"
      ],
      "metadata": {
        "id": "GAlEraYb_smr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "metadata": {
        "id": "vngMLN568mcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing del modello"
      ],
      "metadata": {
        "id": "OipxJ52Y_ry5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_my_examples(inputs, results):\n",
        "    # Creazione di una lista formattata per stampare gli input e i risultati\n",
        "    result_for_printing = [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
        "                           for i in range(len(inputs))]\n",
        "    # Stampa dei risultati formattati\n",
        "    print(*result_for_printing, sep='\\n')\n",
        "    print()\n",
        "\n",
        "# Esempi di frasi da valutare\n",
        "examples = [\n",
        "    'this is such an amazing movie!',  # stessa frase provata in precedenza\n",
        "    'The movie was great!',\n",
        "    'The movie was meh.',\n",
        "    'The movie was okish.',\n",
        "    'The movie was terrible...'\n",
        "]\n",
        "\n",
        "# Valutazione dei risultati utilizzando il modello ricaricato e il modello originale\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
        "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
        "\n",
        "# Stampa dei risultati ottenuti dal modello salvato\n",
        "print('Results from the saved model:')\n",
        "print_my_examples(examples, reloaded_results)\n",
        "\n",
        "# Stampa dei risultati ottenuti dal modello in memoria\n",
        "print('Results from the model in memory:')\n",
        "print_my_examples(examples, original_results)\n"
      ],
      "metadata": {
        "id": "b-pgc8Sx_3V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo codice valuta una serie di esempi di frasi utilizzando due modelli diversi: uno caricato da un modello salvato (reloaded_model) e l'altro presente in memoria (classifier_model). I risultati vengono confrontati e stampati per mostrare le previsioni generate dai due modelli su ciascuna frase di esempio."
      ],
      "metadata": {
        "id": "9d5iyApp_79J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desideri utilizzare il tuo modello su [TF Serving](https://www.tensorflow.org/tfx/guide/serving), ricorda che chiamerà il tuo SavedModel tramite una delle sue firme denominate. In Python, puoi testarli come segue:"
      ],
      "metadata": {
        "id": "TqX5VaRlAICG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "serving_results = reloaded_model \\\n",
        "            .signatures['serving_default'](tf.constant(examples))\n",
        "\n",
        "serving_results = tf.sigmoid(serving_results['classifier'])\n",
        "\n",
        "print_my_examples(examples, serving_results)"
      ],
      "metadata": {
        "id": "UxB5KcqEAKhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valutazione sui dati di test"
      ],
      "metadata": {
        "id": "pvlQzPoXAz4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_entities(text, result, tokenizer):\n",
        "    # Estrai le etichette predette dal risultato del modello\n",
        "    predicted_labels = tf.math.argmax(result, axis=-1).numpy()[0]\n",
        "    # Converti gli ID delle etichette in etichette leggibili\n",
        "    labels = [classifier_model.config.id2label[label_id] for label_id in predicted_labels]\n",
        "    # Estrai i token dal tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Inizializza una lista per memorizzare le entità riconosciute\n",
        "    recognized_entities = []\n",
        "    # Variabile per memorizzare l'entità corrente mentre viene costruita\n",
        "    current_entity = {\"text\": \"\", \"label\": None}\n",
        "\n",
        "    # Itera sui token e sulle relative etichette predette\n",
        "    for token, label in zip(tokens, labels):\n",
        "        # Se l'etichetta inizia con 'B-', inizia una nuova entità\n",
        "        if label.startswith('B-'):\n",
        "            # Se c'è un'entità corrente, aggiungila alla lista\n",
        "            if current_entity[\"text\"]:\n",
        "                recognized_entities.append(current_entity)\n",
        "            # Inizia una nuova entità con il token corrente e l'etichetta\n",
        "            current_entity = {\"text\": token, \"label\": label[2:]}\n",
        "        # Se l'etichetta inizia con 'I-', aggiungi il token all'entità corrente\n",
        "        elif label.startswith('I-'):\n",
        "            current_entity[\"text\"] += \" \" + token\n",
        "        # Se l'etichetta non è né 'B-' né 'I-', controlla se c'è un'entità corrente\n",
        "        # e aggiungila alla lista se presente\n",
        "        else:\n",
        "            if current_entity[\"text\"]:\n",
        "                recognized_entities.append(current_entity)\n",
        "                current_entity = {\"text\": \"\", \"label\": None}\n",
        "\n",
        "    # Aggiungi l'ultima entità corrente alla lista se presente\n",
        "    if current_entity[\"text\"]:\n",
        "        recognized_entities.append(current_entity)\n",
        "\n",
        "    return recognized_entities\n"
      ],
      "metadata": {
        "id": "jmD3c3bWA26X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dati di test vs Dati di Train"
      ],
      "metadata": {
        "id": "11KPmM6JA9BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione del modello sui dati di test\n",
        "test_loss, test_accuracy = classifier_model.evaluate(test_ds)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Valutazione del modello sui dati di addestramento\n",
        "train_loss, train_accuracy = classifier_model.evaluate(train_ds)\n",
        "print(f'Training Loss: {train_loss}')\n",
        "print(f'Training Accuracy: {train_accuracy}')\n"
      ],
      "metadata": {
        "id": "Av_HrgSmA6Ei",
        "outputId": "527ddfb8-81a4-4446-88ae-85e9004d1fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "583/782 [=====================>........] - ETA: 4:24 - loss: 0.9579 - binary_accuracy: 0.4997"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analisys"
      ],
      "metadata": {
        "id": "mNCeICj5Be0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Preprocess the input text\n",
        "    processed_tweet = re.sub(r'\\W', ' ', str(text))\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        "    processed_tweet = re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        "    processed_tweet = processed_tweet.lower()\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "\n",
        "    # Transform the preprocessed text into a TF-IDF array\n",
        "    X = tfidfconverter.fit_transform([processed_tweet]).toarray()\n",
        "\n",
        "    # Load the trained model\n",
        "    text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "    # Predict the sentiment of the text\n",
        "    prediction = text_classifier.predict(X)\n",
        "\n",
        "    # Return the sentiment prediction (positive or negative)\n",
        "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "# Esempio di utilizzo della funzione per analizzare il sentiment di un testo\n",
        "text_example = \"This movie is fantastic!\"\n",
        "sentiment_result = analyze_sentiment(text_example)\n",
        "print(f\"The sentiment of the text '{text_example}' is: {sentiment_result}\")\n"
      ],
      "metadata": {
        "id": "Qit7lxPHBfpj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}