{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdiEyrZeXwvFSy8LTifKU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnrdmnc/NER-NLP/blob/main/spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduzione\n",
        "\n",
        "Questo tutorial fornisce una breve introduzione al lavoro con il linguaggio naturale (talvolta chiamato \"text analytics\") in Python, utilizzando [spaCy](https://spacy.io/) e le librerie correlate.\n",
        "I team che si occupano di scienza dei dati nell'industria devono lavorare con molti testi, una delle quattro principali categorie di dati utilizzati nell'apprendimento automatico.\n",
        "Di solito si tratta di testo generato dall'uomo, anche se non sempre.\n",
        "\n",
        "Pensateci: come funziona il \"sistema operativo\" delle aziende? In genere, ci sono contratti (di vendita, di lavoro, di collaborazione), fatture, polizze assicurative, regolamenti e altre leggi, e così via.\n",
        "Tutti questi elementi sono rappresentati sotto forma di testo.\n",
        "\n",
        "Potreste imbattervi in alcuni acronimi: _elaborazione del linguaggio naturale_ (NLP), _comprensione del linguaggio naturale_ (NLU), _generazione del linguaggio naturale_ (NLG) - che sono rispettivamente \"leggere il testo\", \"comprendere il significato\", \"scrivere il testo\".\n",
        "Sempre più spesso questi compiti si sovrappongono e diventa difficile categorizzare qualsiasi caratteristica.\n",
        "\n",
        "Il framework _spaCy_, insieme a un'ampia e crescente gamma di plug-in e altre integrazioni, fornisce funzionalità per un'ampia gamma di compiti di linguaggio naturale.\n",
        "È diventata una delle librerie di linguaggio naturale più utilizzate in Python per i casi d'uso dell'industria e ha una comunità piuttosto numerosa e, con essa, un grande sostegno per la commercializzazione dei progressi della ricerca, dato che quest'area continua a evolversi rapidamente."
      ],
      "metadata": {
        "id": "N6Vi8otqihM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "[note di installazione](https://spacy.io/usage) per un \"configuratore\" che genera i comandi di installazione in base alle piattaforme e ai linguaggi naturali da supportare.\n",
        "\n",
        "Alcuni tendono a usare `pip` mentre altri usano `conda`, e ci sono istruzioni per entrambi.  Per esempio, per iniziare a usare _spaCy_ che lavora con testi in inglese ed è installato tramite `conda` su un sistema Linux:\n",
        "```\n",
        "conda install -c conda-forge spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "BTW, la seconda riga qui sopra è un download di risorse linguistiche (modelli, ecc.) e il `_sm` alla fine del nome del download indica un modello \"piccolo\". Ci sono anche \"medium\" e \"large\", anche se questi sono piuttosto grandi. Alcune delle funzioni più avanzate dipendono da quest'ultimo, anche se in questo (breve) tutorial non ci immergeremo fino in fondo a questo oceano.\n",
        "\n",
        "Ora carichiamo _spaCy_ ed eseguiamo un po' di codice:"
      ],
      "metadata": {
        "id": "Hu3aleQMiy7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GmZMu1y3EHKq"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variabile nlp è ora la porta d'accesso a tutto ciò che è spaCy ed è caricata con il piccolo modello en_core_web_sm per l'inglese. Quindi, facciamo passare un piccolo \"documento\" attraverso il parser del linguaggio naturale:"
      ],
      "metadata": {
        "id": "9vApJC2JjFgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCD9S9PZjF3q",
        "outputId": "40a61d27-65a0-4a09-c43f-c4d34963df16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The the DET True\n",
            "rain rain NOUN False\n",
            "in in ADP True\n",
            "Spain Spain PROPN False\n",
            "falls fall VERB False\n",
            "mainly mainly ADV False\n",
            "on on ADP True\n",
            "the the DET True\n",
            "plain plain NOUN False\n",
            ". . PUNCT False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per prima cosa abbiamo creato un [doc](https://spacy.io/api/doc) dal testo, che è un contenitore per un documento e tutte le sue annotazioni. Poi abbiamo iterato il documento per vedere cosa _spaCy_ aveva analizzato.\n",
        "\n",
        "Bene, ma sono molte informazioni e un po' difficili da leggere. Riformuliamo l'analisi di _spaCy_ di quella frase come un dataframe [pandas](https://pandas.pydata.org/):"
      ],
      "metadata": {
        "id": "dxYOmcVQjce-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)"
      ],
      "metadata": {
        "id": "quEeRhnYjabq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Molto più leggibile!\n",
        "In questo semplice caso, l'intero documento è solo una breve frase.\n",
        "Per ogni parola della frase, _spaCy_ ha creato un [token](https://spacy.io/api/token), e in ogni token sono stati inseriti dei campi da mostrare:\n",
        "\n",
        " - testo grezzo\n",
        " - [lemma](https://en.wikipedia.org/wiki/Lemma_(morfologia)) - una forma radicale della parola\n",
        " - [parte del discorso](https://en.wikipedia.org/wiki/Part_of_speech)\n",
        " - un flag che indica se la parola è una _stopword_, cioè una parola comune che può essere filtrata"
      ],
      "metadata": {
        "id": "CEzSQN0nj2fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizziamo poi la libreria [displaCy](https://ines.io/blog/developing-displacy) per visualizzare l'albero di analisi della frase:"
      ],
      "metadata": {
        "id": "8LwI58v8j_sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "XQ4pXY8sj5sG",
        "outputId": "ec5159f1-3368-43d0-e7dd-5a897f2a5eee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cc955f13b84042208153c19aad27904d-0\" class=\"displacy\" width=\"1625\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">rain</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Spain</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">falls</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mainly</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">plain.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-6\" stroke-width=\"2px\" d=\"M1295,177.0 C1295,89.5 1445.0,89.5 1445.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,179.0 L1287,167.0 1303,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cc955f13b84042208153c19aad27904d-0-7\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,2.0 1450.0,2.0 1450.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cc955f13b84042208153c19aad27904d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,179.0 L1458.0,167.0 1442.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando _spaCy_ crea un documento, utilizza un principio di _tokenizzazione non distruttiva_ che significa che i token, le frasi, ecc. sono semplicemente indici in un lungo array. In altre parole, non taglia il flusso di testo in piccoli pezzi. Quindi ogni frase è uno [span](https://spacy.io/api/span) con un indice di inizio e uno di fine nell'array del documento:"
      ],
      "metadata": {
        "id": "wtTOk_mRkIM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "    print(\">\", sent.start, sent.end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwgQ7x64kMNd",
        "outputId": "2772a4f5-a191-415b-c8cd-6001c097a5f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 0 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo indicizzare l'array del documento per estrarre i token di una frase:"
      ],
      "metadata": {
        "id": "VTCe1EyBkQeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[48:54]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2jyiQockUcK",
        "outputId": "c655d62e-9779-4235-9bee-5adbce65de19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = doc[51]\n",
        "print(token.text, token.lemma_, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "l6U1kGfRkX_Q",
        "outputId": "0859bd13-02d2-4a7b-bb8c-8b54223246aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "[E026] Error accessing token at position 51: out of bounds in Doc of length 10.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-82b6222d8885>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.bounds_check\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: [E026] Error accessing token at position 51: out of bounds in Doc of length 10."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto possiamo analizzare un documento, segmentarlo in frasi, quindi esaminare le annotazioni sui token di ciascuna frase. È un buon inizio."
      ],
      "metadata": {
        "id": "MCC-yGiFkXu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquisizione del testo\n",
        "\n",
        "Ora che possiamo analizzare i testi, dove li troviamo?\n",
        "Una fonte rapida è quella di sfruttare il web.\n",
        "Naturalmente, quando scarichiamo le pagine web, otteniamo l'HTML e dobbiamo estrarre il testo da esse.\n",
        "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) è un pacchetto popolare per questo scopo.\n",
        "\n",
        "Prima di tutto, un po' di pulizia:"
      ],
      "metadata": {
        "id": "_73LP1-4khUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ZB5TI6X6kh5M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di seguito sono riportati esempi di utilizzo di [codec](https://docs.python.org/3/library/codecs.html) e [normalize unicode](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize). NB: il testo di esempio proviene dall'articolo \"[Metal umlat](https://en.wikipedia.org/wiki/Metal_umlaut)\"."
      ],
      "metadata": {
        "id": "oKVJGeCKkr1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"Rinôçérôse screams ﬂow not unlike an encyclopædia, \\\n",
        "'TECHNICIÄNS ÖF SPÅCE SHIP EÅRTH THIS IS YÖÜR CÄPTÅIN SPEÄKING YÖÜR ØÅPTÅIN IS DEA̋D' to Spın̈al Tap.\"\n",
        "\n",
        "type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZFqZOuQkzPg",
        "outputId": "aa985f96-4b7d-46df-add1-0ffc503007fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variabile x è una stringa in Python:"
      ],
      "metadata": {
        "id": "MEXwSeOXlKqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repr(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mdrbk57Hk7fY",
        "outputId": "b08e4b76-3d06-410a-fad8-0018c52b2adf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Rinôçérôse screams ﬂow not unlike an encyclopædia, \\'TECHNICIÄNS ÖF SPÅCE SHIP EÅRTH THIS IS YÖÜR CÄPTÅIN SPEÄKING YÖÜR ØÅPTÅIN IS DEA̋D\\' to Spın̈al Tap.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sua traduzione in [ASCII](http://www.asciitable.com/) è inutilizzabile dai parser:"
      ],
      "metadata": {
        "id": "eNhMZCh5lswB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ascii(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hvTaTfqiluET",
        "outputId": "46ea4644-9df0-47b6-df23-2471d5e5c9e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Rin\\\\xf4\\\\xe7\\\\xe9r\\\\xf4se screams \\\\ufb02ow not unlike an encyclop\\\\xe6dia, \\'TECHNICI\\\\xc4NS \\\\xd6F SP\\\\xc5CE SHIP E\\\\xc5RTH THIS IS Y\\\\xd6\\\\xdcR C\\\\xc4PT\\\\xc5IN SPE\\\\xc4KING Y\\\\xd6\\\\xdcR \\\\xd8\\\\xc5PT\\\\xc5IN IS DEA\\\\u030bD\\' to Sp\\\\u0131n\\\\u0308al Tap.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La codifica in [UTF-8](http://unicode.org/faq/utf_bom.html) non aiuta molto:\n",
        "Ignorare i caratteri difficili è forse una strategia ancora peggiore:"
      ],
      "metadata": {
        "id": "uxY_nkhxltl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.encode('utf8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeNfF4jolr1i",
        "outputId": "a1bc6dea-7683-4d13-bfd3-429d74696ae3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Rin\\xc3\\xb4\\xc3\\xa7\\xc3\\xa9r\\xc3\\xb4se screams \\xef\\xac\\x82ow not unlike an encyclop\\xc3\\xa6dia, 'TECHNICI\\xc3\\x84NS \\xc3\\x96F SP\\xc3\\x85CE SHIP E\\xc3\\x85RTH THIS IS Y\\xc3\\x96\\xc3\\x9cR C\\xc3\\x84PT\\xc3\\x85IN SPE\\xc3\\x84KING Y\\xc3\\x96\\xc3\\x9cR \\xc3\\x98\\xc3\\x85PT\\xc3\\x85IN IS DEA\\xcc\\x8bD' to Sp\\xc4\\xb1n\\xcc\\x88al Tap.\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignorare i caratteri difficili è forse una strategia ancora peggiore:"
      ],
      "metadata": {
        "id": "8ujgPoGsl0v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.encode('ascii', 'ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-s4MbUml05L",
        "outputId": "5fe1b923-1e7d-4489-aa26-9a8d41ed3b5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Rinrse screams ow not unlike an encyclopdia, 'TECHNICINS F SPCE SHIP ERTH THIS IS YR CPTIN SPEKING YR PTIN IS DEAD' to Spnal Tap.\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuttavia, si può *normalizzare* il testo, quindi codificare..."
      ],
      "metadata": {
        "id": "y1zMaj3ql6eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "unicodedata.normalize('NFKD', x).encode('ascii','ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKPufpEXl4l_",
        "outputId": "49750185-6d57-40d6-fb46-1b41cda9c64b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Rinocerose screams flow not unlike an encyclopdia, 'TECHNICIANS OF SPACE SHIP EARTH THIS IS YOUR CAPTAIN SPEAKING YOUR APTAIN IS DEAD' to Spnal Tap.\""
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anche prima di questa normalizzazione e codifica, potrebbe essere necessario convertire esplicitamente alcuni caratteri **prima** del parsing. Per esempio:"
      ],
      "metadata": {
        "id": "SrIsS4symzmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"The sky “above” the port … was the color of ‘cable television’ – tuned to the Weather Channel®\"\n",
        "ascii(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Bf5Pgb-Ql_IM",
        "outputId": "f8432d35-a30b-4730-ad57-d9f0133f7d07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'The sky \\\\u201cabove\\\\u201d the port \\\\u2026 was the color of \\\\u2018cable television\\\\u2019 \\\\u2013 tuned to the Weather Channel\\\\xae'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerate i risultati di questa riga:"
      ],
      "metadata": {
        "id": "noVEaEmmm6Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unicodedata.normalize('NFKD', x).encode('ascii', 'ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7i0P0S4mFZK",
        "outputId": "7b398e34-2390-486e-b622-11c25887cfcd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'The sky above the port ... was the color of cable television  tuned to the Weather Channel'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...che ancora tralascia caratteri che potrebbero essere importanti per l'analisi di una frase.\n",
        "\n",
        "Quindi un approccio più avanzato potrebbe essere:"
      ],
      "metadata": {
        "id": "d563yB9AmXbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.replace('“', '\"').replace('”', '\"')\n",
        "x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "x = x.replace('…', '...').replace('–', '-')\n",
        "x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8')\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkloQwzmIsG",
        "outputId": "29eff76a-74a8-4a4d-a483-808c1159e610"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky \"above\" the port ... was the color of 'cable television' - tuned to the Weather Channel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing HTML"
      ],
      "metadata": {
        "id": "wRSHGQV7nQfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import traceback\n",
        "\n",
        "def get_text (url):\n",
        "    buf = []\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
        "\n",
        "        for p in soup.find_all(\"p\"):\n",
        "            buf.append(p.get_text())\n",
        "\n",
        "        return \"\\n\".join(buf)\n",
        "    except:\n",
        "        print(traceback.format_exc())\n",
        "        sys.exit(-1)"
      ],
      "metadata": {
        "id": "uAzP9LNWnWzu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora prendiamo alcuni testi da fonti online.\n",
        "Possiamo confrontare le licenze open source ospitate sul sito della [Open Source Initiative](https://opensource.org/licenses/):"
      ],
      "metadata": {
        "id": "ouYSeldAndux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lic = {}\n",
        "lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\n",
        "lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-2.0\"))\n",
        "lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n",
        "\n",
        "for sent in lic[\"bsd\"].sents:\n",
        "    print(\">\", sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdCPp5dPncSz",
        "outputId": "4b7bf15d-a841-4e60-8f19-0a63d67bfd7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Search\n",
            "\n",
            "\n",
            "\t\t\t\t\t\t\t\tSPDX short identifier:\n",
            "\t\t\t\t\t\t\t\tBSD-3-Clause\t\t\t\t\t\t\t\n",
            "\n",
            "Note: This license has also been called the “New BSD License” or “Modified BSD License”.\n",
            "> See also the 2-clause BSD License.\n",
            "\n",
            "> Copyright <YEAR> <COPYRIGHT HOLDER>\n",
            "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
            "1.\n",
            "> Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
            "\n",
            "> 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
            "\n",
            "> 3.\n",
            "> Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
            "\n",
            "> THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND\n",
            "> ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n",
            "> IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n",
            "> HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
            "> ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            " \n",
            "\n",
            "> Mastodon\n",
            "Twitter\n",
            "LinkedIn\n",
            "Reddit\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "> The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License.Opensource.org is not the author of any of the licenses reproduced on this site.\n",
            "> Questions about the copyright in a license should be directed to the license steward.\n",
            " \n",
            "> Proudly powered by WordPress.\n",
            "> Hosted by Pressable. \n",
            "\n",
            "> The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License.Opensource.org is not the author of any of the licenses reproduced on this site.\n",
            "> Questions about the copyright in a license should be directed to the license steward.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un caso d'uso comune per il lavoro con il linguaggio naturale è quello di confrontare i testi. Ad esempio, con le licenze open source possiamo scaricare i testi, analizzarli e poi confrontare le metriche di [similarità](https://spacy.io/api/doc#similarity) tra loro:"
      ],
      "metadata": {
        "id": "x_ZHTESVniYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    [\"mit\", \"asl\"],\n",
        "    [\"asl\", \"bsd\"],\n",
        "    [\"bsd\", \"mit\"]\n",
        "]\n",
        "\n",
        "for a, b in pairs:\n",
        "    print(a, b, lic[a].similarity(lic[b]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5WU0cYQngs0",
        "outputId": "b55df9c3-82e1-4770-93bb-3bb7e56cb83f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mit asl 0.8898028126415602\n",
            "asl bsd 0.8789439210898888\n",
            "bsd mit 0.970449166310577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo è interessante, poiché le licenze [BSD](https://opensource.org/licenses/BSD-3-Clause) e [MIT](https://opensource.org/licenses/MIT) sembrano essere i documenti più simili.\n",
        "In effetti sono strettamente correlate.\n",
        "\n",
        "È vero che in ogni documento è stato incluso del testo aggiuntivo a causa della clausola di esclusione di responsabilità OSI nel piè di pagina, ma questo fornisce un'approssimazione ragionevole per confrontare le licenze."
      ],
      "metadata": {
        "id": "KSLZSnw8nlNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Understanding\n",
        "Ora analizziamo alcune delle caratteristiche più interessanti per la NLU.\n",
        "Dato che abbiamo il parsing di un documento, da un punto di vista puramente grammaticale possiamo estrarre i [noun chunks](https://spacy.io/usage/linguistic-features#noun-chunks), cioè ciascuna delle frasi di sostantivo:"
      ],
      "metadata": {
        "id": "Q4xS261knmsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqtenaSgnm49",
        "outputId": "90e9163c-1739-48de-917e-abb55c0327d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve Jobs\n",
            "Steve Wozniak\n",
            "Apple Computer\n",
            "January\n",
            "Cupertino\n",
            "California\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non male. Le frasi dei sostantivi in una frase forniscono generalmente un maggior contenuto informativo, come un semplice filtro utilizzato per ridurre un lungo documento in una rappresentazione più \"distillata\".\n",
        "\n",
        "Possiamo portare avanti questo approccio e identificare le [named entities](https://spacy.io/usage/linguistic-features#named-entities) all'interno del testo, cioè i nomi propri:"
      ],
      "metadata": {
        "id": "6JrzdYmFoDqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLYiUeUxoD_u",
        "outputId": "ae434688-4030-43c0-a2ab-6793e2633878"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve Jobs PERSON\n",
            "Steve Wozniak PERSON\n",
            "Apple Computer ORG\n",
            "January 3, 1977 DATE\n",
            "Cupertino GPE\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La libreria _displaCy_ offre un modo eccellente per visualizzare le entità denominate:"
      ],
      "metadata": {
        "id": "GUJkRi5coG-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "llww7uLzoHRV",
        "outputId": "d1367515-6ab7-4970-9da5-d67fa04cfe04"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Jobs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Wozniak\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " incorporated \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Computer\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    January 3, 1977\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    California\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se si lavora con applicazioni [knowledge graph](https://www.akbc.ws/) e altri [linked data](http://linkeddata.org/), la sfida consiste nel costruire collegamenti tra le entità nominate in un documento e altre informazioni correlate alle entità, il che si chiama [entity linking](http://nlpprogress.com/english/entity_linking.html).\n",
        "L'identificazione delle entità nominate in un documento è il primo passo di questo particolare tipo di lavoro di intelligenza artificiale.\n",
        "Per esempio, dato il testo qui sopra, si potrebbe collegare l'entità denominata `Steve Wozniak' a un [lookup in DBpedia](http://dbpedia.org/page/Steve_Wozniak)."
      ],
      "metadata": {
        "id": "_jrfAYm0oJ-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In termini più generali, si possono anche collegare i lemmi alle risorse che ne descrivono il significato.\n",
        "Per esempio, in una prima sezione abbiamo analizzato la frase \"I gorilla si sono scatenati\" e siamo riusciti a dimostrare che il lemma della parola \"si sono scatenati\" è il verbo \"andare\". A questo punto possiamo utilizzare un venerabile progetto chiamato [WordNet](https://wordnet.princeton.edu/) che fornisce un database lessicale per l'inglese - in altre parole, è un thesaurus computabile.\n",
        "\n",
        "Esiste un'integrazione _spaCy_ per WordNet chiamata\n",
        "[spacy-wordnet](https://github.com/recognai/spacy-wordnet) di [Daniel Vila Suero](https://twitter.com/dvilasuero), un esperto di linguaggio naturale e di grafi di conoscenza.\n",
        "\n",
        "Poi caricheremo i dati di WordNet tramite NLTK (sono cose che succedono):"
      ],
      "metadata": {
        "id": "HGDiNKeDoNPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fyxqctYoKIm",
        "outputId": "a23ff3a3-ec06-4f04-b79a-a3ae26668c57"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/10cEYZS4I_nua081chK6MHeocGpdLBmSf#scrollTo=Cn3jthvsqn-b&line=1&uniqifier=1"
      ],
      "metadata": {
        "id": "Cn3jthvsqn-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5gvZGBVoNdo",
        "outputId": "158041fd-7623-4f2e-c9e7-00b9a11951f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy-wordnet in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: nltk<3.6,>=3.3 in /usr/local/lib/python3.10/dist-packages (from spacy-wordnet) (3.5)\n",
            "Requirement already satisfied: spacy>=2 in /usr/local/lib/python3.10/dist-packages (from spacy-wordnet) (3.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy-wordnet) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy-wordnet) (1.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy-wordnet) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy-wordnet) (4.66.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy-wordnet) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy-wordnet) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy-wordnet) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy-wordnet) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2->spacy-wordnet) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2->spacy-wordnet) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2->spacy-wordnet) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2->spacy-wordnet) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gb046camoRLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "print(\"before\", nlp.pipe_names)\n",
        "\n",
        "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
        "\n",
        "print(\"after\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "vvTbnfyeoRSb",
        "outputId": "fb6303a1-d64f-4874-de58-8e2eba85ca22"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "WordnetAnnotator.__init__() missing 1 required positional argument: 'name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f7d9bfdd337d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"WordnetAnnotator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordnetAnnotator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tagger\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: WordnetAnnotator.__init__() missing 1 required positional argument: 'name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella lingua inglese, alcune parole sono famose per avere molti significati possibili. Ad esempio, si possono consultare i risultati di una ricerca online su [WordNet](http://wordnetweb.princeton.edu/perl/webwn?s=star&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=) per trovare i significati relativi alla parola `ritirare`.\n",
        "\n",
        "Ora usiamo _spaCy_ per eseguire questa ricerca automaticamente:"
      ],
      "metadata": {
        "id": "dAbdSirRoS4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = nlp(\"withdraw\")[0]\n",
        "token._.wordnet.synsets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "xUg83JfGoTAg",
        "outputId": "3525ceb5-8e59-487a-ef19-16cf629b7388"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "[E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7a5dca566af5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"withdraw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token._.wordnet.lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "5WdJ9wu2oWnQ",
        "outputId": "d4d8e71e-b91d-456c-8c4d-177b4df87775"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "[E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-430dc83efe5c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token._.wordnet.wordnet_domains()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "r_T7hMg2oYMw",
        "outputId": "2f0737b2-b400-4822-9b35-b4d1da7f0d87"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "[E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5f25413809cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ancora una volta, se si lavora con i grafi di conoscenza, i collegamenti al \"senso delle parole\" di WordNet possono essere utilizzati insieme agli algoritmi dei grafi per aiutare a identificare i significati di una particolare parola. Questo può anche essere usato per sviluppare riassunti per sezioni più ampie di testo, attraverso una tecnica chiamata riassunto. Si tratta di una tecnica che esula dagli scopi di questo tutorial, ma che rappresenta un'applicazione interessante per il linguaggio naturale nell'industria.\n",
        "\n",
        "In un'altra direzione, se si sa a priori che un documento riguarda un particolare dominio o un insieme di argomenti, è possibile vincolare i significati restituiti da WordNet. Nell'esempio che segue, vogliamo considerare i risultati della NLU che si riferiscono al settore finanziario e bancario:"
      ],
      "metadata": {
        "id": "kJIyOdFZoZgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domains = [\"finance\", \"banking\"]\n",
        "sentence = nlp(\"I want to withdraw 5,000 euros.\")\n",
        "\n",
        "enriched_sent = []\n",
        "\n",
        "for token in sentence:\n",
        "    # get synsets within the desired domains\n",
        "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
        "\n",
        "    if synsets:\n",
        "        lemmas_for_synset = []\n",
        "\n",
        "        for s in synsets:\n",
        "            # get synset variants and add to the enriched sentence\n",
        "            lemmas_for_synset.extend(s.lemma_names())\n",
        "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
        "    else:\n",
        "        enriched_sent.append(token.text)\n",
        "\n",
        "print(\" \".join(enriched_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "Xp2nBhzxoZuh",
        "outputId": "262ec0ab-3304-479f-cacb-bd4ea995301b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "[E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b1337eef43e6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get synsets within the desired domains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msynsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet_synsets_for_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'wordnet'. Did you forget to call the `set_extension` method?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'esempio può sembrare semplice ma, se si gioca con l'elenco dei domini, si scopre che i risultati hanno una sorta di esplosione combinatoria quando vengono eseguiti senza vincoli ragionevoli. Immaginate di avere un grafo della conoscenza con milioni di elementi: vorreste limitare le ricerche dove possibile per evitare che ogni query richieda giorni/settimane/mesi/anni di calcolo.\n",
        "\n",
        "A volte i problemi che si incontrano quando si cerca di capire un testo - o meglio ancora quando si cerca di capire un corpus (un insieme di dati con molti testi correlati) - diventano così complessi che è necessario prima visualizzarli. Ecco una visualizzazione interattiva per la comprensione dei testi: scattertext, frutto del genio di Jason Kessler. Da installare:\n",
        "\n",
        "```conda install -c conda-forge scattertext```\n",
        "Analizziamo i dati di testo delle convention di partito durante le elezioni presidenziali americane del 2012. L'esecuzione potrebbe richiedere uno o due minuti, ma i risultati di tutta questa elaborazione di numeri valgono l'attesa."
      ],
      "metadata": {
        "id": "mgA9_Bg7oetL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scattertext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR9nyImhoe2H",
        "outputId": "20518274-7acf-45d3-f156-3148519f5946"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.19-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.5.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scattertext) (0.14.1)\n",
            "Collecting flashtext (from scattertext)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from scattertext) (4.3.2)\n",
            "Requirement already satisfied: spacy>=3.2 in /usr/local/lib/python3.10/dist-packages (from scattertext) (3.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scattertext) (4.66.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->scattertext) (6.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scattertext) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext) (3.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scattertext) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels->scattertext) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2->scattertext) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2->scattertext) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2->scattertext) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2->scattertext) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2->scattertext) (2.1.5)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=9df6cbb61176366ef1d318503525d02da08f26744051b296f79551297a9523f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, scattertext\n",
            "Successfully installed flashtext-2.7 scattertext-0.1.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scattertext as st\n",
        "\n",
        "if \"merge_entities\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
        "\n",
        "if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
        "\n",
        "convention_df = st.SampleCorpora.ConventionData2012.get_data()\n",
        "corpus = st.CorpusFromPandas(convention_df,\n",
        "                             category_col=\"party\",\n",
        "                             text_col=\"text\",\n",
        "                             nlp=nlp).build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "bsqdrAWYohTo",
        "outputId": "0ffecaf0-ce81-4998-89e6-0ee94520ddb1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function merge_entities at 0x7e38cbb3fc70> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-aca0625e3926>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"merge_entities\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merge_entities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"merge_noun_chunks\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function merge_entities at 0x7e38cbb3fc70> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una volta pronto il `corpus', generare una visualizzazione interattiva in HTML:\n",
        "\n"
      ],
      "metadata": {
        "id": "voEqKb5aoiXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"democrat\",\n",
        "    category_name=\"Democratic\",\n",
        "    not_category_name=\"Republican\",\n",
        "    width_in_pixels=1000,\n",
        "    metadata=convention_df[\"speaker\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3RZ8cyOcoifZ",
        "outputId": "31d610f3-1195-4674-9a27-2ea5483214d7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'corpus' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-037e36e5d0bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m html = st.produce_scattertext_explorer(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"democrat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcategory_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Democratic\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnot_category_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Republican\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora renderizzeremo l'HTML: dategli un minuto o due per caricarlo, ne vale la pena..."
      ],
      "metadata": {
        "id": "lZVWBAvYolLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "from IPython.core.display import display, HTML\n",
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "print(IN_COLAB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg1WIkBcolR6",
        "outputId": "a53ae0e5-4d01-436a-d4de-717337c813cc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LzvLLi5conTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
        "    display(HTML(html))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "eLRqa4-SonyK",
        "outputId": "3fdfb7c5-0102-482b-ae60-12c1ae776a9e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:98% !important; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'html' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f302a4353460>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<style>.container { width:98% !important; }</style>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'html' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-KtI8I8aopSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"foo.html\"\n",
        "\n",
        "with open(file_name, \"wb\") as f:\n",
        "    f.write(html.encode(\"utf-8\"))\n",
        "\n",
        "IFrame(src=file_name, width = 1200, height=700)"
      ],
      "metadata": {
        "id": "1gPpjYW7opgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}